# -*- coding: utf-8 -*-
"""model_f1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A33uyw7qQL-UAch7FvDK3CgkGvhzKntW
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %run /content/drive/MyDrive/news/library.ipynb
# %run /content/drive/MyDrive/news/utils.ipynb
# %run /content/drive/MyDrive/news/math.ipynb

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

from sklearn.metrics import f1_score

title_data = pd.read_excel('/content/taehee.xlsx')

title_data.loc[(title_data['label'] == "a"), 'label'] = 0  
title_data.loc[(title_data['label'] == "b"), 'label'] = 1  
title_data.loc[(title_data['label'] == "c"), 'label'] = 2

title_list = []
for title, label in zip(title_data['title'], title_data['label'])  :
    data = []
    data.append(title)
    data.append(str(label))

    title_list.append(data)

test_size = 0.25
random_state = 42
max_len = 64
batch_size = 40
warmup_ratio = 0.1
num_epochs = 15
max_grad_norm = 1
log_interval = 200
learning_rate =  6e-6

title_train, title_test = train_test_split(title_list, test_size=test_size, random_state=random_state)

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)
#sentidx,label_idx, bert_tokenizer, max_len, pad, pair
train = BERTDataset(title_train, 0, 1, tok, max_len, True, False)
test = BERTDataset(title_test, 0, 1, tok, max_len, True, False)

train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)

class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=3,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)


no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()

t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)

scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

def f1score(X,Y) :
    Y = Y.cpu().numpy()
    X = X.detach().cpu().numpy()
    lab_list = []
    out_list = []
    for i in range(len(out)):
      lab = max(X[i])
      if lab == X[i][0]:
        lab = 0
      elif lab == X[i][1]:
        lab = 1
      elif lab == X[i][2]:
        lab = 2
      out_list.append(lab)
    for i in range(len(Y)):
      lab_list.append(Y[i])
    f1 = f1_score(out_list,lab_list,average='macro')

    return f1

for e in range(num_epochs):
    train_f1_score = 0.0
    test_f1_score = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)

        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step() 

        train_f1_score += f1score(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} f1 score {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_f1_score / (batch_id+1)))
    print("epoch {} train f1 score {}".format(e+1, train_f1_score / (batch_id+1)))
    
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)

        test_f1_score += f1score(out, label)
    print("epoch {} test f1 score {}".format(e+1, test_f1_score / (batch_id+1)))

naver_news = load_files('/content/drive/MyDrive/newsData', shuffle=True)

# Commented out IPython magic to ensure Python compatibility.
# %run '/content/drive/MyDrive/Colab Notebooks/news_category.ipynb'



list_0 = []
list_1 = []
list_2 = ['']

model_cate.predict()

#토큰화
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

def predict(predict_sentence):

    data = [predict_sentence, '0']
    dataset_another = [data]

    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=2)
    
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)
        # for i in range(len(list_0)):
        #   if list_0[i] in predict_sentence :
        #     out += 0.5
        if cat_pred(predict_sentence) == '연예' or '스포츠':
          out += 0.5

        test_eval = []
        for i in out:
            
            logits=i
            logits = np.round(logits.detach().cpu().numpy(),2)
            
            for idx , j in enumerate(logits):
                logits[idx] = np.exp(j)
            tot_e = sum(logits)
            logits = (logits/tot_e) * 100
            #print(type(logits))
            if np.argmax(logits) == 0:
                test_eval.append("판단유보")
            elif np.argmax(logits) == 1:
                test_eval.append("책임회피")
            else : test_eval.append("선정주의")

        
        print(">> 이타이틀은 판단유보 {:.2f}%, 책임회피 {:.2f}%, 선정주의 {:.2f}% 으로 측정되어 {}유형의 따옴표입니다".format(logits[0],logits[1],logits[2],test_eval[0]))
        return logits

test_data = pd.read_excel('/content/testplus.xlsx')
test_list = []
for title in test_data['title'] :
    data = []
    data.append(title)

    test_list.append(data)

for i in range(len(test_list)):
  print(test_list[i][0])
  predict(test_list[i][0])
  print('\n')

def predict_score(predict_sentence):

    data = [predict_sentence, '0']
    dataset_another = [data]

    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=2)
    
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)

        test_eval = []
        for i in out:
            
            logits=i
            logits = np.round(logits.detach().cpu().numpy(),2)
            
            for idx , j in enumerate(logits):
                logits[idx] = np.exp(j)
            tot_e = sum(logits)
            logits = (logits/tot_e) * 100
            #print(type(logits))
            if np.argmax(logits) == 0:
                test_eval.append("0")
            elif np.argmax(logits) == 1:
                test_eval.append("1")
            else : test_eval.append("2")

        
        #print(">> 이타이틀은 판단유보 {:.2f}%, 책임회피 {:.2f}%, 선정주의 {:.2f}% 으로 측정되어 {}유형의 따옴표입니다".format(logits[0],logits[1],logits[2],test_eval[0]))

        return test_eval[0]

y_true = []
for i in range(len(title_test)) :
  y_true.append(int(title_test[i][1]))

y_pred = []
for i in range(len(title_test)) :
  y_pred.append(int(predict_score(title_test[i][0])))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

confusion_matrix(y_true,y_pred)
print(classification_report(y_true,y_pred))

"""Precision:- Accuracy of positive predictions.

Precision = TP/(TP + FP)

Recall:- Fraction of positives that were correctly identified.

Recall = TP/(TP+FN)

F1 score

F1 Score = 2*(Recall * Precision) / (Recall + Precision)


Accuracy : (TP+TN) / all


macro avg = (normal+abnormal) /2 * precision or recall or f1 score

weighted avg = normal/(normal+abnormal)  *  precision or recall or f1 score
"""



# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import re
import pickle
import numpy as np
import pandas as pd
from scipy import sparse
import sklearn
import sklearn.ensemble
import sklearn.metrics
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

from metal.label_model import LabelModel
from metal.analysis import lf_summary, label_coverage
from metal.label_model.baselines import MajorityLabelVoter

from lime import lime_text

pip install lime

from lime import lime_text

from sklearn.pipeline import make_pipeline

from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=['0', '1','2'])

predict(title_test[0][0])

# Commented out IPython magic to ensure Python compatibility.
# Install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers
# Import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd
# Import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import FeatureExtractionPipeline, BertModel, BertTokenizer, BertConfig
from lime.lime_text import LimeTextExplainer
# Let the notebook know to plot inline
# %matplotlib inline



